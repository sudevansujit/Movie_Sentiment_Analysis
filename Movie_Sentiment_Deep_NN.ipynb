{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie_Sentiment_Deep_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFOubMisQ9EyELPH9dS6N5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudevansujit/Movie_Sentiment_Analysis/blob/master/Movie_Sentiment_Deep_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPS8Avmg92Uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzD6aMOb94UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geCFm-xD96Vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1asZyLVRZ3QjWG4Pek-dAYYl9Kq9XZn3n'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Cleaned_Movie_Review.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZJJh943-EL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "756b9977-51fc-4ea9-b261-44d3d84534cc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('Cleaned_Movie_Review.csv', index_col = 'Unnamed: 0')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>new_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "      <td>one reviewers mentioned watching oz episode ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production the filming tech...</td>\n",
              "      <td>1</td>\n",
              "      <td>wonderful little production filming technique ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "      <td>thought wonderful way spend time hot summer we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically there s a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "      <td>basically family little boy jake thinks zombie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter mattei s love in the time of money is a...</td>\n",
              "      <td>1</td>\n",
              "      <td>petter mattei s love time money visually stunn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                                         new_review\n",
              "0  one of the other reviewers has mentioned that ...  ...  one reviewers mentioned watching oz episode ho...\n",
              "1  a wonderful little production the filming tech...  ...  wonderful little production filming technique ...\n",
              "2  i thought this was a wonderful way to spend ti...  ...  thought wonderful way spend time hot summer we...\n",
              "3  basically there s a family where a little boy ...  ...  basically family little boy jake thinks zombie...\n",
              "4  petter mattei s love in the time of money is a...  ...  petter mattei s love time money visually stunn...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEvl29vU-QWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e58b3db-fb2e-45c1-c269-6c133cb0d895"
      },
      "source": [
        "# build train and test datasets\n",
        "X_review = data['new_review'].values\n",
        "y_sentiment = data['sentiment'].values\n",
        "\n",
        "X_train = X_review[:35000]\n",
        "y_train = y_sentiment[:35000]\n",
        "\n",
        "X_test = X_review[35000:]\n",
        "y_test = y_sentiment[35000:]\n",
        "\n",
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000,), (35000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IrO96OE-bRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cbfae6c1-c3d6-42e0-bb7b-67e67ec0f940"
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFWMqxFH-vYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize train reviews \n",
        "tokenized_train = [nltk.word_tokenize(text) for text in X_train]\n",
        "\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text) for text in X_test]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-ox6sB_-ylV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Engineering with word embeddingsÂ¶\n",
        "w2v_num_features = 300\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, \n",
        "                                   size=w2v_num_features, \n",
        "                                   window=150,\n",
        "                                   min_count=10, \n",
        "                                   workers=4, \n",
        "                                   iter=5)   "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujQvUJqe_CU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl-xW5UnADuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcbaca0d-4a21-45b5-ce32-2bd62abf5d04"
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, \n",
        "                                                     model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)\n",
        "\n",
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (35000, 300)  Test features shape: (15000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqLjacX6AIOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Modeling with deep neural networks\n",
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(1))\n",
        "    dnn_model.add(Activation('sigmoid'))\n",
        "\n",
        "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANIVgOhuATeQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "d37c8b54-3db5-41fb-d08e-f90660790824"
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)\n",
        "\n",
        "w2v_dnn.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 351,489\n",
            "Trainable params: 351,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wdsSC23AWs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "cce9cf1c-8b41-447d-83ed-7ccfd4cdb7e8"
      },
      "source": [
        "batch_size = 100\n",
        "history = w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "315/315 [==============================] - 3s 10ms/step - loss: 0.3062 - accuracy: 0.8722 - val_loss: 0.2918 - val_accuracy: 0.8726\n",
            "Epoch 2/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2819 - accuracy: 0.8855 - val_loss: 0.2840 - val_accuracy: 0.8794\n",
            "Epoch 3/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2739 - accuracy: 0.8870 - val_loss: 0.2917 - val_accuracy: 0.8766\n",
            "Epoch 4/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2674 - accuracy: 0.8889 - val_loss: 0.2870 - val_accuracy: 0.8803\n",
            "Epoch 5/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2590 - accuracy: 0.8939 - val_loss: 0.2871 - val_accuracy: 0.8791\n",
            "Epoch 6/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2503 - accuracy: 0.8966 - val_loss: 0.2863 - val_accuracy: 0.8786\n",
            "Epoch 7/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2420 - accuracy: 0.9001 - val_loss: 0.2917 - val_accuracy: 0.8757\n",
            "Epoch 8/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2339 - accuracy: 0.9027 - val_loss: 0.2938 - val_accuracy: 0.8834\n",
            "Epoch 9/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2252 - accuracy: 0.9066 - val_loss: 0.2980 - val_accuracy: 0.8803\n",
            "Epoch 10/10\n",
            "315/315 [==============================] - 3s 9ms/step - loss: 0.2134 - accuracy: 0.9114 - val_loss: 0.3035 - val_accuracy: 0.8800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhKHEgy6AhKy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cb0c157-8900-4e61-ce12-571fe4e220db"
      },
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(\"% Accuracy = \",accuracy_score(y_test, y_pred)*100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% Accuracy =  88.02666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP9WmcYJAwR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1c1bb9d6-608c-41fb-ba69-b926f48450e4"
      },
      "source": [
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6657  833]\n",
            " [ 963 6547]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88      7490\n",
            "           1       0.89      0.87      0.88      7510\n",
            "\n",
            "    accuracy                           0.88     15000\n",
            "   macro avg       0.88      0.88      0.88     15000\n",
            "weighted avg       0.88      0.88      0.88     15000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uamFtTSjA_ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thanks"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_C0NoL8BC4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}